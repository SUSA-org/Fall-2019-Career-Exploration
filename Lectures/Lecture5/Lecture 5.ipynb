{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Linear Algebra and Linear Regression\n",
    "\n",
    "## 10/20/2019\n",
    "\n",
    "### Table of Contents\n",
    "* [Linear Algebra](#linear_algebra)  \n",
    "    * [Motivation](#motivation)  \n",
    "    * [Notation](#notation)  \n",
    "    * [Matrix Terminology](#terminology) \n",
    "    * [Matrix Operations](#operation) \n",
    "        * [Transpose](#transpose)\n",
    "        * [Addition & Subtraction](#addsub)\n",
    "        * [Scalar Multiplication](#scalar_mul)\n",
    "        * [Matrix Multiplication](#matrix_mul)\n",
    "    * [Linear Independence](#linear_independence) \n",
    "    * [Invertibility](#invert) \n",
    "    * [Matrix Operations in numpy](#Matrix-Operations-in-numpy)\n",
    "        * [Scaler Multiplication](#Scaler-Multiplication)\n",
    "        * [Matrix Multiplication](#Matrix-Multiplication)\n",
    "        * [Matrix Inverse](#Matrix-Inverse) \n",
    "        * [Matrix Linear Independence](#Matrix-Linear-Independence) \n",
    "        * [Matrix Transpose](#Matrix-Transpose)\n",
    "\n",
    "* [Regression](#regression)\n",
    "    * [Introduction](#intro)\n",
    "        * [What Is A Model?](#what_model)\n",
    "        * [Why Make A Model?](#why_model)\n",
    "    * [Linear Regression](#linear_regression)\n",
    "        * [Model Assumptions](#assumptions)\n",
    "        * [Simple Linear Regression](#simple)\n",
    "        * [Introduction to Least Square Error](#loss)\n",
    "        * [Alternative error functions](#Alternative-error-functions)\n",
    "        * [Minimizing the Error](#ols)\n",
    "        * [Making the Model](#making_model)\n",
    "        * [Interpreting the Model](#interpreting_model)\n",
    "        * [Assessing the Model](#assessment)\n",
    "            * [Coefficient of Determination ($R^2$)](#r_squared)\n",
    "            * [Residual Plots](#residual_plots)\n",
    "            * [Q-Q Plots](#qq_plots)\n",
    "        * [When Can I Use A Linear Model?](#when_to_use)\n",
    "        * [Multiple Linear Regression](#multiple)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Student Association for Applied Statistics (SAAS)](https://susa.berkeley.edu). Authored by [Thomas Glezen](http://tcglezen.com) and [Yiming Shi](mailto:ys1998@berkeley.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_algebra'></a>\n",
    "# Linear Algebra\n",
    "\n",
    "<a id='motivation'></a>\n",
    "## Motivation\n",
    "Linear algebra is the study of vector spaces, and it encompasses linear equations and functions represented by vector spaces and matrices. Vectors and matrices are essential for storing data, which is why we often use Python packages such as Numpy and Pandas. A common problem is linear regression, which we will delve into solving using matrices and linear algebra.\n",
    "\n",
    "<a id='notation'></a>\n",
    "## Notation\n",
    "Common notation:\n",
    "\n",
    "* $\\mathbf{A}$: Bold capital letters represent matrices\n",
    "* $\\mathbf{x}$: Bold lowercase letters represent vectors\n",
    "* $\\theta$: Non-bold values represent scalars\n",
    "\n",
    "<a id='terminology'></a>\n",
    "## Matrix Terminology\n",
    "* **Identity** matrix: A square matrix with diagonal elements equal to $1$ and all off diagonal elements equal to zero. A $n\\times n$ identity matrix is often denoted as $I$ or $I_n$.\n",
    "* **Order or Size** of matrix: If a matrix has m rows and n columns, the order of the matrix is $m\\times n$. We denote the set of (real-valued) matrices $\\mathbb{M}_{m,n}$\n",
    "* **Transpose** of a matrix: The transpose of matrix $\\mathbf{A}$ satisfies the condition $\\mathbf{A_{j,i}} = \\mathbf{A_{i,j}}^T$. That is, the first row of $\\mathbf{A}$ is the first column of $\\mathbf{A}^T$.\n",
    "* **Square** matrix: A matrix with the same number of rows as columns. This matrix is in the shape of a square.\n",
    "* **Diagonal** matrix: A matrix with all the non-diagonal elements equal to $0$ is called a diagonal matrix.\n",
    "* **Scalar** matrix: A square matrix with all the diagonal elements equal to a constant and non-diagonal elements equal to 0.\n",
    "* **Column** matrix: A matrix which consists of exactly $1$ column. If it has $m$ rows, it can be treated as a $m\\times 1$ vector. \n",
    "* **Row** matrix: A matrix which consists of exactly $1$ row. If it has $m$ columns, it can be treated as a $1\\times m$ vector. \n",
    "\n",
    "<a id='operation'></a>\n",
    "## Matrix Operations\n",
    "\n",
    "<a id='transpose'></a>\n",
    "### Transpose\n",
    "Transpose converts row vectors into column vectors, and vice versa. For example, for a $3 \\times 2$ matrix $\\mathbf{A}$ where $$ \\mathbf{A} = \\begin{bmatrix} 0 & 4\\\\ 7 & 0 \\\\ 3 & 1\\end{bmatrix}, \\mathbf{A}^T = \\begin{bmatrix} 0 & 7 & 3\\\\ 4 & 0 & 1\\end{bmatrix}$$ \n",
    "\n",
    "Note that $$(\\mathbf{A}^T)^T = \\mathbf{A}$$\n",
    "\n",
    "<a id='addsub'></a>\n",
    "### Addition & Subtraction\n",
    "if $\\mathbf{A}$ and $\\mathbf{B}$ are both $m \\times n$, we form $\\mathbf{A} + \\mathbf{B}$ by adding corresponding entries. $$\\mathbf{A} = \\begin{bmatrix} a_1 & a_2\\\\ a_3 & a_4 \\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix} b_1 & b_2\\\\ b_3 & b_4 \\end{bmatrix}, \\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_1+b_1 & a_2+b_2\\\\ a_3+b_3 & a_4+b_4 \\end{bmatrix}$$ A fact about matrix addition and transpose is that $(\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T$.\n",
    "\n",
    "Similarly, we can perform subtraction in the same way, where we form $\\mathbf{A} - \\mathbf{B}$ by subtracting entries of $\\mathbf{B}$ from corresponding entries of $\\mathbf{A}$.\n",
    "\n",
    "Note that we can only perform matrix addition and subtraction when both matrices in the operation have the same dimension.\n",
    "\n",
    "\n",
    "<a id='scalar_mul'></a>\n",
    "### Scalar Multiplication\n",
    "We can multiply a scalar (a.k.a. number) by a matrix by multiplying every entry of the matrix by the scalar. This is denoted $\\cdot$ between the scalar and the matrix. $$c \\cdot \\begin{bmatrix} a_1 & a_2\\\\ a_3 & a_4 \\end{bmatrix} = \\begin{bmatrix} c\\times a_1 & c\\times a_2\\\\ c\\times a_3 & c\\times a_4 \\end{bmatrix}$$\n",
    "\n",
    "<a id='matrix_mul'></a>\n",
    "### Matrix Multiplication\n",
    "If $\\mathbf{A}$ is $m \\times p$ and $\\mathbf{B}$ is $p \\times n$, we can form $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, which has dimension $m \\times n$. For matrix multiplication to occur, the number of columns of $\\mathbf{A}$ has to equal the number of rows of $\\mathbf{B}$. A fact about matrix multiplication and transpose is that $(\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T$.$$\\begin{bmatrix} a_{11} & a_{12} & a_{13}\\\\ a_{21} & a_{22} & a_{23} \\end{bmatrix}\\begin{bmatrix} b_{11} & b_{12}\\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_{1\\cdot}}\\times \\mathbf{b_{\\cdot1}} & \\mathbf{a_{1\\cdot}}\\times \\mathbf{b_{\\cdot2}}\\\\ \\mathbf{a_{2\\cdot}}\\times \\mathbf{b_{\\cdot1}} & \\mathbf{a_{2\\cdot}}\\times \\mathbf{b_{\\cdot2}} \\end{bmatrix}$$ where $\\mathbf{a_{i\\cdot}}$ is the $i$th row of matrix $\\mathbf{A}$ and $\\mathbf{b_{\\cdot j}}$ is the $j$th column of matrix $\\mathbf{B}.$\n",
    "\n",
    "For matrix multiplication, in general we don't have $\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='linear_independence'></a>\n",
    "## Linear Independence\n",
    "What does it mean for a set of vectors to be **linearly independent**? It is easier to define what it means to be linearly dependent. A set of vectors $$\\mathbf{x_1, x_2, ..., x_n}$$ is linearly dependent if there exist scalars ${\\alpha_1, \\alpha_2, ..., \\alpha_n}$, not all equal to 0 such that $$\\sum_{i=1}^n \\alpha_ix_i = 0.$$\n",
    "In words, this means that there exists at least one vector that can be written as a linear combination of the remaining vectors.\n",
    "\n",
    "<a id='invert'></a>\n",
    "## Invertibility\n",
    "The inverse of an $n\\times n$ matrix $\\mathbf{A}$, denoted as $\\mathbf{A}^{-1}$, satisfies the following properties:\n",
    "\n",
    "$$\\mathbf{A A}^{-1} = I_{n\\times n},\\ \\mathbf{A}^{-1} \\mathbf{A}=I_{n\\times n}.$$\n",
    "We may consider a concrete example with a $2\\times 2$ matrix. \n",
    "$$\\mathbf{A} = \\begin{bmatrix} a & b\\\\ c & d\\end{bmatrix}$$\n",
    "$$\\mathbf{A}^{-1} = (\\det \\mathbf{A})^{-1}\\begin{bmatrix} d & -b\\\\ -c & a\\end{bmatrix} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b\\\\ -c & a\\end{bmatrix}$$\n",
    "\n",
    "For inverting matrices of higher dimensions, the calculation is much more difficult. We will often want to use a computer to compute these for us. But why do we care about inverse matrices in the first place? Why do they come up in linear regression?\n",
    "\n",
    "The answer to this will show up toward the end of this lecture when we consider the normal equations.\n",
    "\n",
    "Remember for later: If a matrix $\\mathbf{A}$ is invertible, so is its transpose, and the inverse of $\\mathbf{A}^T$ is the transpose of the inverse of $\\mathbf{A}$: $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Operations in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several of the important things that we are going to go over how to code include scaler multiplication, matrix multiplication, matrix inverses, and matrix transposing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import some useful libraries \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's declare a couple of matrices to play with \n",
    "\n",
    "A = np.array([[-24, -18, 5],\n",
    "              [20, -15, -4],\n",
    "              [-5, 4, 1]])\n",
    "\n",
    "B = np.array([[16, -3, -8], \n",
    "              [-10, 15, 4],\n",
    "              [-9, 4, 1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaler Multiplication\n",
    "First, let's do an example of scaler multiplication. It works just like normal multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this example with multiplying one the matrices by a scaler \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Multiplication \n",
    "\n",
    "Next, write down matrix multiplication. Unfortunately, it is slightly different from scaler multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace this line with the product of A and B \n",
    "\n",
    "# Next, try out the product of B and A. Remember, order matters! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Inverse\n",
    "Now we are going to try out inverses. We are going to be using some linear algebra functions from the numpy function. \n",
    "\n",
    "These functions are accessed by calling numpy.linalg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to find the inverse of matrix A. Notice how easy this is compared to doing it by hand\n",
    "\n",
    "\n",
    "\n",
    "# Observe how the results are not exact. Sometimes there are rounding errors when the computer is calculating \n",
    "# the inverse of a matrix. This can resolved by rounding (you can use np.matrix.round(yourMatrixHere)) or simply \n",
    "# ignored since it is such a small error. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Linear Independence \n",
    "\n",
    "However, as we learned earlier, not all matricies are invertible. How can we tell if a matrix is invertible or not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some possibly relevent code can by typed here \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Transpose \n",
    "\n",
    "Lastly, let's try to find the transpose of a matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's transpose that matrix!!! \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='regression'></a>\n",
    "# Regression\n",
    "\n",
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "Your boss is willing to do anything to get into the new hype train of *_big data_* and asks you to use your super duper important data science skills to see if there is any proof that cars that travel faster tend to cover more distance. Your boss gives you a bunch of data involving the speed of a car as well as the distance of a car and it is your job to find some association. (Don't worry, at least you are getting paid well for this.)\n",
    "\n",
    "<a id='what_model'></a>\n",
    "### What Is A Model?\n",
    "A model is a simplification of the real world. We would like the make a prediction for what the real world is actually like by attmepting to \"fit\" the data to an equation. \n",
    "\n",
    "Below are a few (3) attempts to generalize the data into a line of the form y = a + bx \n",
    "\n",
    "<table bgcolor=white><tr>\n",
    "    <td><img src='scatterAndLines.png' width=800 /></td>\n",
    "</tr></table>\n",
    "\n",
    "\n",
    "<a id='why_model'></a>\n",
    "### Why Make A Model?\n",
    "We often find that in the real world, many problems can be (accurately) described by models created by you, the data scientist. You can make a model as an attempt to describe the relationship between many related things, such as the year a computer was made as well as how much memory it can hold. With enough data, you can begin to make predictions such as how much memory a computer may hold in 5 years (hence, the create of [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law))\n",
    "\n",
    "However, it is important to keep in mind what you are analyzing. Just because you can make a model describing the relationship between two variables, and even if you can use this model to predict the value of one variable based on the value of the other, it doesn't mean that one causes the other. You may have heard this before as the difference between **correlation** and **causation**. A classic example is the relationship between ice cream sales and murder rates. Turns out, when ice cream sales rise, so do murder rates. Does this mean ice cream *causes* people to commit murder? Or get murdered? Nope!\n",
    "\n",
    "Today, we're going to learn how to make a linear model to describe the relationship between variables.\n",
    "\n",
    "\n",
    "<a id='linear_regression'></a>\n",
    "## Linear Regression\n",
    "**Linear regression** is a method of making linear models. Linear model is one kind of model, in which the relationship between the explanatory variables and the response variable can be described by a linear function. For now, you can just think of a linear function as a straight line, which takes us to *simple linear regression*.\n",
    "\n",
    "<a id='assumptions'></a>\n",
    "### Model Assumptions\n",
    "Before we go into details about linear regression, let's take a look at the keys assumptions made in linear modeling.\n",
    "* **Linearity**: The relationship between the features and response variables is linear\n",
    "* **Homoscedasticity**: The variance of the error terms is the same for all values of the explanatory variables\n",
    "* **Independence**: The features are linearly independent (The feature matrix has full rank)\n",
    "* **Normality**: The error terms follow a normal distribution\n",
    "\n",
    "<a id='simple'></a>\n",
    "### Simple Linear Regression\n",
    "**Simple linear regression** is a special case of linear regression in which you only have one explanatory variable. As the name suggests, it models the relationship as a *line*. You may be familiar with the slope-intercept form of a line, and that's exactly how the linear model looks!\n",
    "\n",
    "$$y = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "Here, $y$ is the **response** or **dependent** variable we're trying to predict, and $x$ is an **explanatory** or **independent** variable used to predict $y$. For example, when you're trying to predict your friend's weight, $y$ would represent weight, while $x$ represents height.\n",
    "\n",
    "Using known $x$'s, we want to accurately predict $y$ using the right $\\theta_0$ and $\\theta_1$.\n",
    "\n",
    "\\*\\* I know that seeing $\\theta$'s everywhere can be quiet scary. It is helpful to keep in mind that these variables are constants and can be replaced with any other variable in the alphabet that makes you comfortable, such as a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='loss'></a>\n",
    "### Introduction to Least Square Error \n",
    "\n",
    "In the previous examples, all of the provided lines were examples of attempts to model the data. The question comes to mind: \"How do we numerically decide which line is the best line?\" As it turns out, there are many different methods that can be used to measure \"how bad\" each line is. (For those interested, it is known as a \"Loss Function,\" and more about it can be read [here](https://en.wikipedia.org/wiki/Loss_function).)\n",
    "\n",
    "The loss function that we will be teaching in the class is known as \"least square error.\" The general spirit of this method is that if we have a bunch of predicted values, $\\hat{y_1}, \\hat{y_2} .. \\hat{y_n}$ and a bunch of predicted values, $y_1, y_2 ... y_n$, we can generate a heuristic by calculating (incoming SUPER important equation) \n",
    "$$\\sum_{i = 1}^{n} \\big( y_{i} - \\hat{y_i} \\big)^{2}$$\n",
    "\n",
    "If you remember, $\\hat{y_i}$ is our prediction, which can otherwise be expressed as $\\theta_0 + \\theta_1 x_i$. By substituting in the expression, we get \n",
    "\n",
    "$$\\sum_{i = 1}^{n} \\big( y_{i} - ( \\theta_0 + \\theta_1 x_i) \\big)^{2}$$\n",
    "\n",
    "It's great to see that we have a method of measuring how good (or bad) linear model is (by the expression above), but how do we find _the best_ model? \n",
    "\n",
    "In order to find the best model, we need to find some value for $\\theta_0$ and $\\theta_1$ that minimizes the expression above. \n",
    "\n",
    "\n",
    "<img src='simple_linear.png' width=400>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative error functions \n",
    "\n",
    "Can you think of why we would want to *square* the residuals and sum them instead of just minimizing their sum? i.e. Why do we minimize $\\sum_{i=0}^n (y_i - \\theta_0-\\theta_1x_i)^2$ instead of $\\sum_{i=0}^n (y_i - \\theta_0-\\theta_1x_i)$?\n",
    "\n",
    "And why do we prefer using *squared* residuals instead of taking their *absolute value*? i.e. Why do we prefer to minimize $\\sum_{i=0}^n (y_i - \\theta_0-\\theta_1x_i)^2$ instead of $\\sum_{i=0}^n |y_i - \\theta_0-\\theta_1x_i|$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ols'></a>\n",
    "### Minimizing the Error\n",
    "Our goal is to minimize the **sum of squared residual** or the  sum of the squared difference between the predicted value and the observed value for a given $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to minimize the **residual sum of squares (RSS)**, what we're actually going to minimize is this:\n",
    "\n",
    "$$\\textit{RSS} = \\sum_{i=0}^n {e_i}^2 = \\sum_{i=0}^n (y_i - \\theta_0-\\theta_1x_i)^2$$\n",
    "\n",
    "By minimizing this function, we can solve for slope $\\theta_1$ and the intercept $\\theta_0$. The actual calculations for deriving the formulas that define these coefficients requires a bit of calculus. Since calculus is a bit harder to do on the computer (it is possible!), so we'll be doing this part on paper! If you would like an online reference, feel free to click [this link](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf)! For now, we'll just tell you that $\\theta_1$ and $\\theta_0$ can be solved as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\hat{\\theta_1}&=\\bar {y}-\\hat{\\theta_0}\\,{\\bar{x}},\\\\\n",
    "\\hat{\\theta_0}&=\\frac{\\sum _{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar {y})}{\\sum _{i=1}^{n}(x_{i}-\\bar{x})^2}\\\\\n",
    "\\end{aligned}$$\n",
    "\n",
    "This is pretty complicated! Luckily, you don't need to know any of this to make a linear model, but this is here for reference if you're interested in the math behind what we'll be getting into today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='making_model'></a>\n",
    "### Making the Model\n",
    "In linear regression, the response variable is usually continuous. The explanatory variables *can* be discrete and even categorical, but for simple linear regression they are usually continuous. For today we'll just be working with continuous variables! \n",
    "\n",
    "Let's revisit the `titanic` dataset you're familiar with and decide whether it's appropriate for making a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some useful imports \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import overfittingDemo, plot_multiple_linear_regression\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the titanic data from excel sheet\n",
    "titanic_df = pd.read_csv('titanic/train.csv')\n",
    "\n",
    "# plotting ticket class vs. fare\n",
    "titanic_df.plot.scatter(\"Pclass\", \"Fare\")\n",
    "plt.title(\"Titanic: Ticket Class vs. Fare\")\n",
    "plt.xlabel(\"Ticket Class\")\n",
    "plt.ylabel(\"Fare\")\n",
    "\n",
    "# plotting age vs. fare\n",
    "titanic_df.plot.scatter(\"Age\", \"Fare\")\n",
    "plt.title(\"Titanic: Age vs. Fare\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Fare\")\n",
    "\n",
    "# plotting number of siblings vs. fare\n",
    "titanic_df.plot.scatter(\"SibSp\", \"Fare\")\n",
    "plt.title(\"Titanic: # of Siblings/Spouses vs. Fare\")\n",
    "plt.xlabel(\"# of Siblings/Spouses\")\n",
    "plt.ylabel(\"Fare\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How would you describe these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to find the new dataset we found to work with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mpg = pd.read_csv(\"./mpg.csv\", index_col=\"name\") # load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(int) # remove columns with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0)\n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've chosen the `mpg` dataset, which tells us various attributes of different cars, including a car's make and model, miles per gallon, number of cylinders, weight, and more! We're going to be trying to see which features affect a car's `mpg`, and our goal is to create a model that accurately predicts `mpg` given other attributes of the car. \n",
    "\n",
    "You'll notice that we separated the `mpg` data into two separate dataframes, `mpg_train` and `mpg_test`. We'll get into why in next lecture, but for now, make sure to do all of your analysis and model creation on the `mpg_train` dataset! \n",
    "\n",
    "\n",
    "*Hint:* Hitting `shift-tab` with the cursor on the name of a function will bring up helpful documentation about how to use the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are looking at the dat a for cars, what could be a useful variable to predict?\n",
    "Furthermore, to predict this variable, what explanatory or response variable should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = ...\n",
    "y1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_train.plot.scatter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn`'s `linear_model` module makes it really easy to make linear models! There's a lot of different types of linear models implemented in the `linear_model` module, which you can take a look at [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) if you're interested, but for today we'll be using `LinearRegression`, which we've imported for you in the cell below. Try reading the documentation to figure out what the `fit()` function expects as input to correctly fit our model to the `mpg_train` data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X = ...\n",
    "Y = ...\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got it working you'll notice that it seems like nothing happened. However, behind the scenes, our `linear_model` variable has now been fit to the data we passed into the `fit()` function! We can see what the `slope` and `intercept` are by looking into the `coef_` and `intercept_` attributes of our `linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.coef_, linear_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that, while the `intercept_` is a single scalar value, `coef_` returns an array. This is because you can choose to fit your model to multiple explanatory variables (hence the list form of `feature_cols`). When you define multiple explanatory variables, the `coef_` will contain a separate coefficient for each explanatory variable you chose! You'll be able to explore that in a bit, but for now let's take a look at what our linear model looks like relative to our original data.\n",
    "\n",
    "We've provided the skeleton for a helper function called `overlay_simple_linear_model`. Try to fill out the function so that it plots a scatterplot with the linear model overlaid on top.\n",
    "\n",
    "*Hint:* If you press `tab` after a `[object].` or `[package].`, Jupyter will show you a list of valid functions defined for that object type or package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_simple_linear_model(data, x_name, y_name, linear_model):\n",
    "    \"\"\"\n",
    "    This function plots a simple linear model on top of the scatterplot of the data it was fit to.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model\n",
    "    \n",
    "    returns None but outputs linear model overlaid on scatterplot\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(max(data[x_name])).reshape(-1, 1) # a 2D array of integers between 0 and the maximum value of the x_name column\n",
    "    y = linear_model.___ # replace ___ with correct function \n",
    "    \n",
    "    \n",
    "    data.plot.scatter(...) # scatter plot of x_name vs. y_name\n",
    "    \n",
    "    plt.plot(...)\n",
    "    plt.title(\"Linear Model vs. Data: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you wrote the function above correctly, the output should look like this\n",
    "overlay_simple_linear_model(mpg_train, ..., \"mpg\", linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interpreting_model'></a>\n",
    "### Interpreting the Model\n",
    "\n",
    "You're probably thinking \"COOL! This looks like a pretty good representation of the data! But what do these coefficients even mean?\" That is a great question! As you might have guessed, the `intercept` term is where our line intersects with the y-axis, or when our predictor variable has a value of 0. In relation to our model, it's our prediction for `mpg` given a predictor variable value of 0. The `slope` term is a little more complicated. Yes, it is the slope of the line, but how do we interpret it in the relationship between `mpg` and our explanatory variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assessment'></a>\n",
    "### Assessing the Model\n",
    "<a id='r_squared'></a>\n",
    "#### Coefficient of Determination ($R^2$)\n",
    "Another question you might have is, how do we know how good our model is? One way of measuring how well your model fits the data is the $R^2$ coefficient, or the **coefficient of determination**. Basically, what the $R^2$ represents is how much our data can vary but still be predicted accurately by the explanatory variable. $$R^2 = 1- \\frac{\\sum_{i}e_i^2}{\\sum_{i}(y_i-\\bar{y})^2}$$ Intuitively, $R^2$ measures how much better our fitted model is doing compared to the most basic model where we predict $\\hat{y}_i = \\bar{y}$ for every data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain our model's $R^2$ value by using our `linear_model`'s `score()` function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model.score(...) # you'll only need to use variables that we've already defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo! If you used `displacement`, our model accurately predicts 66% of the variation in `mpg`. In practice, $R^2$ is almost always between 0 and 1, although it is possible for $R^2$ to take on a negative value when your model is worse than the baseline model $\\hat{y}_i = \\bar{y}$.\n",
    "\n",
    "**Question:** What does it mean for $R^2$ to have a value of 1? What about 0?\n",
    "\n",
    "\n",
    "**Exercise:** Can you think of a possible feature you could use to make our model have an $R^2$ value of $1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model2 = LinearRegression()\n",
    "\n",
    "X2 = ...\n",
    "\n",
    "linear_model2.fit(X2, Y)\n",
    "\n",
    "r_squared = ...\n",
    "\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='residual_plots'></a>\n",
    "#### Residual Plots\n",
    "Another way of analyzing your model is through *residual plots*. A **residual plot** is kind of what you'd think – it plots your residuals against the corresponding $x$ values. If you see interesting patterns in your residual plot, it's indicative of some *bias* in your model – your error isn't due to randomness in the data but because of an underlying problem in the way you've defined the relationship between your variables. \n",
    "\n",
    "Fill in the blanks in the `plot_simple_residuals()` function, so we can take a look at the residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_residuals(data, x_name, y_name, linear_model):\n",
    "    \"\"\"\n",
    "    This function plots a residual plot based off of a simple linear model \n",
    "    on top of the scatterplot of the data it was fit to.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model\n",
    "    \n",
    "    returns None but outputs residual plot resulting from linear model overlaid on scatterplot\n",
    "    \"\"\"\n",
    "    X = ...\n",
    "    Y = ...\n",
    "    residuals = ...\n",
    "    \n",
    "    ... # plot residuals\n",
    "    plt.axhline(y=0, color='r', linestyle='-') # plots line at y = 0\n",
    "    plt.title(\"Residual Plot: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple_residuals(mpg_train, ..., 'mpg', linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the residuals aren't scattered randomly around the y-axis. The points are more spread out vertically for smaller values of `displacement` and less scattered vertically for larger values. Furthermore, in the middle the residuals are mostly above the line, while on the left and right side, the residuals tend to be below the line. Such a pattern as this one suggests that our model isn't that great at describing the relationship between `displacement` and `mpg`, and there's some fundamental issue with the assumption that the relationship can be modeled by a simple linear relationship. [Here](http://docs.statwing.com/interpreting-residual-plots-to-improve-your-regression/)'s some more information about how to interpret different patterns in residual plots and how you can change your model to fix these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='qq_plots'></a>\n",
    "#### Q-Q Plots\n",
    "A **Q-Q plot** (quantile-quantile plot) is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line that’s roughly straight. Here’s an example of a Normal Q-Q plot when both sets of quantiles truly come from standard normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "nsample = 100\n",
    "np.random.seed(134)\n",
    "x = stats.norm.rvs(loc=0, scale=1, size=nsample) # generating 100 data points from a standard normal distribution\n",
    "fig = sm.qqplot(x, line = '45') # plot the theoretical line where all points should be close to\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Q plot is very useful for checking the normality assumption of linear regression: by plotting the distribution of the data at hand against the distribution of the ideal normal variable, we are able to observe how closely our data follows a normal distribution. If roughly all the points lie on the 45$^{\\circ}$ line, then we assume the normality assumption is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='when_to_use'></a>\n",
    "### When Can I Use a Linear Model?\n",
    "Let's talk about some of the assumptions of linear regression, so you know when it's appropriate to use one. \n",
    "- There's a linear relationship between the response variable and the explanatory variables.\n",
    "- There's no pattern in the residual plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you're a master of simple linear regression, you're probably thinking \"WHY CAN'T I USE MORE EXPLANATORY VARIABLES? What if I think `mpg` could be better predicted if I knew *two* of the variables? Wouldn't that make my model better?\" Why, Ms/Mr. Genius Statistician, you *can* use more explanatory variables! That leads us to *multiple linear regression*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='multiple'></a>\n",
    "## Multiple Linear Regression\n",
    "**Multiple linear regression** is an extension to the simple linear regression model with multiple explanatory variables instead of just one. Note that this is usually the case in reality: for example, your wage doesn't just depend on your education level.\n",
    "\n",
    "With two explanatory variables, we can still visualize the model in a three-dimensional graph, but as we add more and more variables it's pretty much impossible to plot it (can you imagine what a 5D graph would look like?). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're in the d-dimensional world now, we have a model $f_{\\boldsymbol{\\theta}}(x_1, x_2, ..., x_d)$. We would still like to model a single scalar output $y_i$. We continue our assumption that our model is linear, meaning we know that $f_{\\boldsymbol{\\theta}}$ looks like this:\n",
    "$$f_{\\boldsymbol{\\theta}}(x_1, x_2, ..., x_d) = \\theta_0+\\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_d x_d$$\n",
    "\n",
    " Because mathematicians are efficient (not lazy), we can write $f_\\theta$ as an inner product, where $\\mathbf{\\theta}$ is now a **vector** and $x$ is now a $d$-dimensional vector:\n",
    "$$f_{\\boldsymbol{\\theta}}(x) = \\mathbf{x}^T \\boldsymbol{\\theta}$$\n",
    " \n",
    "We can consider each observation $\\mathbf{x}_i$ as a vector of length $d+1$, with 1 added for the intercept term $\\theta_0$. Notice that now we are attempting to calculate $d+1$ values in $\\mathbf{\\theta}$ instead of 2 values previously.  With our current model, to find the optimal $\\mathbf{\\theta}$ we are minimizing\n",
    "$$\\sum_{i=1}^n (y_i - \\mathbf{x}_i^T \\boldsymbol{\\theta})^2 = (\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\boldsymbol{\\theta}})^T(\\mathbf{y}-\\mathbf{X}\\boldsymbol{\\boldsymbol{\\theta}}),$$ where $$\\mathbf{X} = \\begin{bmatrix} \\ 1 & x_{11} & \\dots & x_{1,d} \\\\ 1 & x_{21} & \\dots & x_{2,d} \\\\ \\vdots & \\vdots & \\vdots & \\vdots \\\\ 1 & x_{n1} & \\dots & x_{n,d} \\end{bmatrix}; \\mathbf{\\theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\\\ \\vdots \\\\ \\theta_d \\end{bmatrix}; \\mathbf y = \\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{bmatrix}$$\n",
    "\n",
    "The derivation requires matrix calculus so we'll skip it for now. You can check [here](https://en.wikipedia.org/wiki/Linear_regression#Simple_and_multiple_linear_regression) if you're interested. After some math, we get that the optimal $\\theta$ takes the following form: $$\\boldsymbol{\\theta} = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_model = LinearRegression()\n",
    "X3 = mpg_train[[\"displacement\", \"weight\"]] # select both the displacement and weight columns from mpg_train\n",
    "multiple_model.fit(X3, Y)\n",
    "\n",
    "print(\"Multiple Linear Regression R^2:\", multiple_model.score(X3, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: Is this an $R^2$ value we want? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispX = ... # select just displacement from mpg_train\n",
    "wtX = ...# select just weight from mpg_train\n",
    "\n",
    "linear_model.fit(dispX, Y)\n",
    "print(\"Simple Linear Regression (displacement) R^2:\", linear_model.score(dispX, Y))\n",
    "\n",
    "linear_model.fit(wtX, Y)\n",
    "print(\"Simple Linear Regression (weight) R^2:\", linear_model.score(wtX, Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question**: What do you notice about the $R^2$ values of the simple linear regression models and the multiple case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources/References\n",
    "- [Matplotlib Tutorial - Nicolas P. Rougier](https://www.labri.fr/perso/nrougier/teaching/matplotlib/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
